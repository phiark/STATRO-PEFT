#!/usr/bin/env python3
"""
GPT-2 çƒŸæµ‹è„šæœ¬ - éªŒè¯STRATO-PEFTæ¡†æ¶åŸºç¡€åŠŸèƒ½

è¯¥è„šæœ¬æ‰§è¡ŒåŸºç¡€çš„åŠŸèƒ½æµ‹è¯•ï¼ŒéªŒè¯ï¼š
1. æ¨¡å‹åŠ è½½
2. LoRAé€‚é…å™¨åº”ç”¨
3. åŸºç¡€è®­ç»ƒå¾ªç¯
4. ä¿å­˜å’ŒåŠ è½½

Author: STRATO-PEFT Research Team
Date: 2024
"""

import os
import sys
import logging
import traceback
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent / "src"))

import torch
from omegaconf import OmegaConf
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def test_imports():
    """æµ‹è¯•å…³é”®æ¨¡å—å¯¼å…¥"""
    logger.info("ğŸ§ª æµ‹è¯•æ¨¡å—å¯¼å…¥...")
    
    try:
        from src.peft.lora import LoRALayer
        logger.info("âœ… LoRAæ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        from src.peft.strato_peft import StratoPEFT
        logger.info("âœ… STRATO-PEFTæ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        from src.models.model_factory import ModelFactory
        logger.info("âœ… ModelFactoryå¯¼å…¥æˆåŠŸ")
        
        from src.tasks.task_factory import TaskFactory
        logger.info("âœ… TaskFactoryå¯¼å…¥æˆåŠŸ")
        
        from src.utils.config_utils import validate_config
        logger.info("âœ… ConfigUtilså¯¼å…¥æˆåŠŸ")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_config_loading():
    """æµ‹è¯•é…ç½®åŠ è½½"""
    logger.info("ğŸ§ª æµ‹è¯•é…ç½®åŠ è½½...")
    
    try:
        config_path = "configs/gpt2_smoke_test.yaml"
        config = OmegaConf.load(config_path)
        logger.info("âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
        
        # éªŒè¯é…ç½®ç»“æ„
        required_sections = ['experiment', 'model', 'task', 'peft', 'training']
        for section in required_sections:
            if section not in config:
                raise ValueError(f"é…ç½®ç¼ºå°‘å¿…éœ€çš„section: {section}")
        
        logger.info("âœ… é…ç½®ç»“æ„éªŒè¯é€šè¿‡")
        return config
        
    except Exception as e:
        logger.error(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
        traceback.print_exc()
        return None

def test_model_loading(config):
    """æµ‹è¯•æ¨¡å‹åŠ è½½"""
    logger.info("ğŸ§ª æµ‹è¯•æ¨¡å‹åŠ è½½...")
    
    try:
        # åŠ è½½tokenizer
        model_name = config.model.name
        logger.info(f"åŠ è½½tokenizer: {model_name}")
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir=config.model.get('cache_dir', './cache/models')
        )
        
        # æ·»åŠ pad tokenå¦‚æœä¸å­˜åœ¨
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        logger.info("âœ… TokenizeråŠ è½½æˆåŠŸ")
        
        # åŠ è½½æ¨¡å‹
        logger.info(f"åŠ è½½æ¨¡å‹: {model_name}")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            cache_dir=config.model.get('cache_dir', './cache/models'),
            torch_dtype=torch.float32,  # ä½¿ç”¨float32é¿å…å…¼å®¹æ€§é—®é¢˜
            device_map="auto" if torch.cuda.is_available() else None
        )
        
        logger.info("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        logger.info(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        
        return model, tokenizer
        
    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        traceback.print_exc()
        return None, None

def test_lora_application(model, config):
    """æµ‹è¯•LoRAé€‚é…å™¨åº”ç”¨"""
    logger.info("ğŸ§ª æµ‹è¯•LoRAåº”ç”¨...")
    
    try:
        from src.peft.lora import LoRAAdapter
        
        # åˆ›å»ºLoRAé…ç½®
        lora_config = config.peft.lora
        
        # è®¾ç½®å¿…è¦çš„é…ç½®å­—æ®µ
        lora_config.rank = lora_config.r  # å°†ræ˜ å°„ä¸ºrank
        lora_config.target_modules = lora_config.target_modules
        
        # åº”ç”¨LoRA
        adapter = LoRAAdapter(config=lora_config, model=model)
        adapted_model = adapter.apply_peft()
        
        # æ£€æŸ¥LoRAå‚æ•°
        trainable_params = adapter.get_trainable_parameters()
        total_params = sum(p.numel() for p in model.parameters())
        trainable_count = sum(p.numel() for p in trainable_params)
        
        logger.info(f"âœ… LoRAåº”ç”¨æˆåŠŸ")
        logger.info(f"æ€»å‚æ•°: {total_params:,}")
        logger.info(f"å¯è®­ç»ƒå‚æ•°: {trainable_count:,}")
        logger.info(f"å¯è®­ç»ƒæ¯”ä¾‹: {trainable_count/total_params*100:.2f}%")
        
        return adapted_model, adapter
        
    except Exception as e:
        logger.error(f"âŒ LoRAåº”ç”¨å¤±è´¥: {e}")
        
        # å¦‚æœLoRAAdapterä¸å­˜åœ¨ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨LoRALayer
        try:
            from src.peft.lora import LoRALayer
            logger.info("å°è¯•ä½¿ç”¨LoRALayerç›´æ¥æµ‹è¯•...")
            
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ªçº¿æ€§å±‚è¿›è¡Œæµ‹è¯• (GPT2çš„æ³¨æ„åŠ›å±‚)
            # å…ˆæ‰“å°æ‰€æœ‰çº¿æ€§å±‚åç§°ç”¨äºè°ƒè¯•
            linear_modules = []
            for name, module in model.named_modules():
                if isinstance(module, torch.nn.Linear):
                    linear_modules.append(name)
            logger.info(f"å‘ç°çš„çº¿æ€§å±‚: {linear_modules[:5]}...")  # åªæ˜¾ç¤ºå‰5ä¸ª
            
            for name, module in model.named_modules():
                if isinstance(module, torch.nn.Linear) and ('attn' in name or 'mlp' in name or 'c_' in name):
                    logger.info(f"åœ¨ {name} ä¸Šæµ‹è¯•LoRAå±‚")
                    
                    # åˆ›å»ºLoRAå±‚
                    lora_layer = LoRALayer(
                        original_layer=module,
                        rank=config.peft.lora.r,
                        alpha=config.peft.lora.alpha,
                        dropout=config.peft.lora.dropout
                    )
                    
                    # æµ‹è¯•å‰å‘ä¼ æ’­
                    test_input = torch.randn(1, 10, module.in_features)
                    if torch.cuda.is_available():
                        test_input = test_input.cuda()
                        lora_layer = lora_layer.cuda()
                    
                    with torch.no_grad():
                        output = lora_layer(test_input)
                    
                    logger.info(f"âœ… LoRAå±‚æµ‹è¯•æˆåŠŸï¼Œè¾“å‡ºå½¢çŠ¶: {output.shape}")
                    return model, None
                    
            logger.error("âŒ æœªæ‰¾åˆ°é€‚åˆçš„çº¿æ€§å±‚è¿›è¡ŒLoRAæµ‹è¯•")
            return None, None
            
        except Exception as e2:
            logger.error(f"âŒ LoRAå±‚æµ‹è¯•ä¹Ÿå¤±è´¥: {e2}")
            traceback.print_exc()
            return None, None

def test_data_loading(config, tokenizer):
    """æµ‹è¯•æ•°æ®åŠ è½½"""
    logger.info("ğŸ§ª æµ‹è¯•æ•°æ®åŠ è½½...")
    
    try:
        # ç®€åŒ–çš„æ•°æ®åŠ è½½
        logger.info("åŠ è½½wikitext-2æ•°æ®é›†...")
        
        dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="train")
        
        # å–å°‘é‡æ ·æœ¬
        max_samples = config.task.get('max_samples', 100)
        dataset = dataset.select(range(min(len(dataset), max_samples)))
        
        logger.info(f"âœ… æ•°æ®é›†åŠ è½½æˆåŠŸï¼Œæ ·æœ¬æ•°: {len(dataset)}")
        
        # æµ‹è¯•tokenization
        def tokenize_function(examples):
            return tokenizer(
                examples["text"],
                truncation=True,
                padding="max_length",
                max_length=config.data.max_seq_length,
                return_tensors="pt"
            )
        
        # tokenizeå‰å‡ ä¸ªæ ·æœ¬
        test_samples = dataset.select(range(min(5, len(dataset))))
        tokenized = test_samples.map(tokenize_function, batched=True)
        
        logger.info("âœ… æ•°æ®tokenizationæµ‹è¯•æˆåŠŸ")
        
        return dataset
        
    except Exception as e:
        logger.error(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        traceback.print_exc()
        return None

def test_basic_training_step(model, tokenizer, config):
    """æµ‹è¯•åŸºç¡€è®­ç»ƒæ­¥éª¤"""
    logger.info("ğŸ§ª æµ‹è¯•åŸºç¡€è®­ç»ƒæ­¥éª¤...")
    
    try:
        # åˆ›å»ºç®€å•çš„è®­ç»ƒæ•°æ®
        text = "This is a test sentence for GPT-2 training."
        inputs = tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            padding="max_length",
            max_length=64
        )
        
        if torch.cuda.is_available():
            inputs = {k: v.cuda() for k, v in inputs.items()}
            model = model.cuda()
        
        # è®¾ç½®labels
        inputs["labels"] = inputs["input_ids"].clone()
        
        # å‰å‘ä¼ æ’­
        model.train()
        outputs = model(**inputs)
        loss = outputs.loss
        
        logger.info(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸï¼Œloss: {loss.item():.4f}")
        
        # æµ‹è¯•åå‘ä¼ æ’­
        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        logger.info("âœ… åå‘ä¼ æ’­æˆåŠŸ")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ è®­ç»ƒæ­¥éª¤å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_model_generation(model, tokenizer):
    """æµ‹è¯•æ¨¡å‹ç”Ÿæˆ"""
    logger.info("ğŸ§ª æµ‹è¯•æ¨¡å‹ç”Ÿæˆ...")
    
    try:
        prompt = "The quick brown fox"
        inputs = tokenizer(prompt, return_tensors="pt")
        
        if torch.cuda.is_available():
            inputs = {k: v.cuda() for k, v in inputs.items()}
        
        model.eval()
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=inputs["input_ids"].shape[1] + 10,
                num_return_sequences=1,
                temperature=0.8,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        logger.info(f"âœ… ç”ŸæˆæˆåŠŸ: {generated_text}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹ç”Ÿæˆå¤±è´¥: {e}")
        traceback.print_exc()
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹STRATO-PEFT GPT-2çƒŸæµ‹...")
    
    # æµ‹è¯•ç»“æœç»Ÿè®¡
    tests_passed = 0
    total_tests = 0
    
    # 1. æµ‹è¯•å¯¼å…¥
    total_tests += 1
    if test_imports():
        tests_passed += 1
    
    # 2. æµ‹è¯•é…ç½®åŠ è½½
    total_tests += 1
    config = test_config_loading()
    if config is not None:
        tests_passed += 1
    else:
        logger.error("âŒ é…ç½®åŠ è½½å¤±è´¥ï¼Œç»ˆæ­¢æµ‹è¯•")
        return
    
    # 3. æµ‹è¯•æ¨¡å‹åŠ è½½
    total_tests += 1
    model, tokenizer = test_model_loading(config)
    if model is not None and tokenizer is not None:
        tests_passed += 1
    else:
        logger.error("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œç»ˆæ­¢æµ‹è¯•")
        return
    
    # 4. æµ‹è¯•LoRAåº”ç”¨
    total_tests += 1
    adapted_model, adapter = test_lora_application(model, config)
    if adapted_model is not None:
        tests_passed += 1
        model = adapted_model  # ä½¿ç”¨é€‚é…åçš„æ¨¡å‹
    
    # 5. æµ‹è¯•æ•°æ®åŠ è½½
    total_tests += 1
    dataset = test_data_loading(config, tokenizer)
    if dataset is not None:
        tests_passed += 1
    
    # 6. æµ‹è¯•åŸºç¡€è®­ç»ƒæ­¥éª¤
    total_tests += 1
    if test_basic_training_step(model, tokenizer, config):
        tests_passed += 1
    
    # 7. æµ‹è¯•æ¨¡å‹ç”Ÿæˆ
    total_tests += 1
    if test_model_generation(model, tokenizer):
        tests_passed += 1
    
    # æ€»ç»“
    logger.info("=" * 50)
    logger.info(f"ğŸ¯ çƒŸæµ‹å®Œæˆ: {tests_passed}/{total_tests} æµ‹è¯•é€šè¿‡")
    
    if tests_passed == total_tests:
        logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼STRATO-PEFTæ¡†æ¶åŸºç¡€åŠŸèƒ½æ­£å¸¸")
        return True
    else:
        logger.warning(f"âš ï¸  æœ‰ {total_tests - tests_passed} ä¸ªæµ‹è¯•å¤±è´¥")
        return False

if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except Exception as e:
        logger.error(f"âŒ çƒŸæµ‹è¿‡ç¨‹ä¸­å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {e}")
        traceback.print_exc()
        sys.exit(1)