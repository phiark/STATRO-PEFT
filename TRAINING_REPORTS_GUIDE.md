# è®­ç»ƒæŠ¥å‘Šå’Œç»“æœæ–‡ä»¶ä½ç½®æŒ‡å—

## ğŸ“Š è®­ç»ƒæŠ¥å‘Šå­˜å‚¨ä½ç½®

### é»˜è®¤è¾“å‡ºç»“æ„
```
./results/
â”œâ”€â”€ {experiment_name}_{timestamp}/          # å®éªŒç›®å½•
â”‚   â”œâ”€â”€ config.yaml                         # å®éªŒé…ç½®å¤‡ä»½
â”‚   â”œâ”€â”€ logs/                              # è®­ç»ƒæ—¥å¿—
â”‚   â”‚   â”œâ”€â”€ training.log                   # è¯¦ç»†è®­ç»ƒæ—¥å¿—
â”‚   â”‚   â”œâ”€â”€ evaluation.log                 # è¯„ä¼°æ—¥å¿—
â”‚   â”‚   â””â”€â”€ system.log                     # ç³»ç»Ÿèµ„æºæ—¥å¿—
â”‚   â”œâ”€â”€ checkpoints/                       # æ¨¡å‹æ£€æŸ¥ç‚¹
â”‚   â”‚   â”œâ”€â”€ checkpoint-epoch-1/
â”‚   â”‚   â”œâ”€â”€ checkpoint-epoch-2/
â”‚   â”‚   â””â”€â”€ best-model/
â”‚   â”œâ”€â”€ metrics/                           # è®­ç»ƒæŒ‡æ ‡
â”‚   â”‚   â”œâ”€â”€ training_metrics.json         # è®­ç»ƒè¿‡ç¨‹æŒ‡æ ‡
â”‚   â”‚   â”œâ”€â”€ evaluation_results.json       # æœ€ç»ˆè¯„ä¼°ç»“æœ
â”‚   â”‚   â””â”€â”€ perplexity_history.json       # å›°æƒ‘åº¦å˜åŒ–å†å²
â”‚   â”œâ”€â”€ plots/                             # å¯è§†åŒ–å›¾è¡¨
â”‚   â”‚   â”œâ”€â”€ loss_curve.png
â”‚   â”‚   â”œâ”€â”€ perplexity_curve.png
â”‚   â”‚   â””â”€â”€ learning_rate_schedule.png
â”‚   â””â”€â”€ final_report.html                  # å®Œæ•´è®­ç»ƒæŠ¥å‘Š
```

### å…³é”®æŒ‡æ ‡æ–‡ä»¶

#### 1. å›°æƒ‘åº¦å’ŒæŸå¤± (`metrics/training_metrics.json`)
```json
{
  "epoch_1": {
    "train_loss": 2.456,
    "train_perplexity": 11.65,
    "eval_loss": 2.234,
    "eval_perplexity": 9.34,
    "learning_rate": 0.0002
  },
  "epoch_2": {
    "train_loss": 2.123,
    "train_perplexity": 8.35,
    "eval_loss": 2.045,
    "eval_perplexity": 7.73,
    "learning_rate": 0.00018
  }
}
```

#### 2. æœ€ç»ˆè¯„ä¼°ç»“æœ (`metrics/evaluation_results.json`)
```json
{
  "final_perplexity": 7.23,
  "final_loss": 1.978,
  "accuracy": 0.856,
  "parameter_efficiency": 0.9997,
  "training_time": 1845.6,
  "inference_speed": 125.3,
  "memory_usage_mb": 4832.1
}
```

#### 3. PEFTç‰¹å®šæŒ‡æ ‡
```json
{
  "lora_metrics": {
    "trainable_params": 18432,
    "total_params": 124458240,
    "trainable_ratio": 0.000148,
    "rank_distribution": {
      "transformer.h.0.attn.c_attn": 8,
      "transformer.h.0.attn.c_proj": 8
    }
  },
  "strato_metrics": {
    "exploration_episodes": 15,
    "best_configuration": {
      "transformer.h.0.attn.c_attn": 12,
      "transformer.h.1.attn.c_attn": 8
    },
    "reward_history": [0.23, 0.45, 0.67, 0.82],
    "cost_efficiency": 0.956
  }
}
```

## ğŸ” å¦‚ä½•æŸ¥çœ‹å’Œåˆ†ææŠ¥å‘Š

### å¿«é€ŸæŸ¥çœ‹æœ€æ–°ç»“æœ
```bash
# æŸ¥çœ‹æœ€è¿‘çš„å®éªŒç»“æœ
ls -la ./results/ | head -10

# æŸ¥çœ‹ç‰¹å®šå®éªŒçš„å›°æƒ‘åº¦
cat ./results/gpt2_lora_experiment_20240614/metrics/evaluation_results.json | jq '.final_perplexity'

# æŸ¥çœ‹è®­ç»ƒè¿‡ç¨‹
tail -f ./results/latest_experiment/logs/training.log
```

### ä½¿ç”¨å†…ç½®æŠ¥å‘Šå·¥å…·
```bash
# ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
python scripts/generate_report.py --experiment_dir ./results/your_experiment

# æ¯”è¾ƒå¤šä¸ªå®éªŒ
python scripts/compare_experiments.py --experiments_dir ./results

# å¯¼å‡ºä¸ºCSV
python scripts/export_metrics.py --experiment_dir ./results/your_experiment --format csv
```

## ğŸ“ˆ è‡ªåŠ¨æŠ¥å‘Šç”Ÿæˆ

æ¡†æ¶ä¼šè‡ªåŠ¨ç”Ÿæˆï¼š
1. **å®æ—¶ç›‘æ§**: WandBä»ªè¡¨æ¿ (å¦‚æœå¯ç”¨)
2. **ç»ˆç«¯è¾“å‡º**: è®­ç»ƒè¿‡ç¨‹ä¸­çš„å®æ—¶æŒ‡æ ‡
3. **HTMLæŠ¥å‘Š**: å®Œæ•´çš„å®éªŒæŠ¥å‘Š (`final_report.html`)
4. **JSONæŒ‡æ ‡**: æœºå™¨å¯è¯»çš„è¯¦ç»†æŒ‡æ ‡
5. **å¯è§†åŒ–å›¾è¡¨**: è®­ç»ƒæ›²çº¿å’Œæ€§èƒ½å›¾è¡¨

## ğŸ¯ å¿«é€Ÿå®šä½å…³é”®æŒ‡æ ‡

| ä½ æƒ³æŸ¥çœ‹çš„æŒ‡æ ‡ | æ–‡ä»¶ä½ç½® | å…³é”®å­—æ®µ |
|---------------|----------|----------|
| æœ€ç»ˆå›°æƒ‘åº¦ | `metrics/evaluation_results.json` | `final_perplexity` |
| è®­ç»ƒæŸå¤±å†å² | `metrics/training_metrics.json` | `train_loss` |
| å‚æ•°æ•ˆç‡ | `metrics/evaluation_results.json` | `parameter_efficiency` |
| è®­ç»ƒæ—¶é—´ | `metrics/evaluation_results.json` | `training_time` |
| å†…å­˜ä½¿ç”¨ | `metrics/evaluation_results.json` | `memory_usage_mb` |
| LoRAé…ç½® | `config.yaml` | `peft.lora` |
| é”™è¯¯æ—¥å¿— | `logs/training.log` | grep "ERROR" |