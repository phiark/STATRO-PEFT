#!/usr/bin/env python3
"""
æ£€æŸ¥GPT2æ¨¡å‹ç»“æ„çš„è„šæœ¬
"""

import torch
from transformers import AutoModelForCausalLM

def inspect_gpt2_modules():
    """è¯¦ç»†æ£€æŸ¥GPT2çš„æ¨¡å—ç»“æ„"""
    model = AutoModelForCausalLM.from_pretrained("gpt2")
    
    print("ğŸ” GPT2æ¨¡å‹çš„è¯¦ç»†ç»“æ„:")
    print("=" * 80)
    
    # å…ˆæ˜¾ç¤ºæ‰€æœ‰æ¨¡å—
    print("æ‰€æœ‰æ¨¡å—:")
    all_modules = []
    for name, module in model.named_modules():
        all_modules.append((name, type(module).__name__))
    
    # æ˜¾ç¤ºå‰20ä¸ªæ¨¡å—æ¥äº†è§£ç»“æ„
    for i, (name, type_name) in enumerate(all_modules[:20]):
        print(f"  {name} -> {type_name}")
    if len(all_modules) > 20:
        print(f"  ... è¿˜æœ‰ {len(all_modules) - 20} ä¸ªæ¨¡å—")
    
    print("\n" + "-" * 80)
    
    linear_modules = []
    conv1d_modules = []
    
    # æ£€æŸ¥Linearå’ŒConv1Då±‚
    from transformers.pytorch_utils import Conv1D
    
    for name, module in model.named_modules():
        if isinstance(module, torch.nn.Linear):
            linear_modules.append({
                'name': name,
                'in_features': module.in_features,
                'out_features': module.out_features,
                'bias': module.bias is not None,
                'type': 'Linear'
            })
        elif isinstance(module, Conv1D):
            conv1d_modules.append({
                'name': name,
                'in_features': module.weight.shape[0],
                'out_features': module.weight.shape[1], 
                'bias': module.bias is not None,
                'type': 'Conv1D'
            })
    
    all_linear_like = linear_modules + conv1d_modules
    
    print(f"æ‰¾åˆ° {len(linear_modules)} ä¸ªLinearå±‚ å’Œ {len(conv1d_modules)} ä¸ªConv1Då±‚:")
    print("-" * 80)
    
    # æ˜¾ç¤ºæ‰€æœ‰çº¿æ€§å±‚ç±»å‹çš„æ¨¡å—
    for i, mod in enumerate(all_linear_like):
        print(f"{i+1:2d}. {mod['name']} ({mod['type']})")
        print(f"    è¾“å…¥: {mod['in_features']}, è¾“å‡º: {mod['out_features']}, bias: {mod['bias']}")
    
    print("\n" + "=" * 80)
    print("ğŸ¯ æ¨èçš„LoRA target_modules:")
    
    # æ³¨æ„åŠ›æ¨¡å—
    attention_modules = [mod['name'] for mod in all_linear_like if 'attn' in mod['name']]
    print(f"æ³¨æ„åŠ›æ¨¡å— ({len(attention_modules)}ä¸ª):")
    for mod in attention_modules[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
        print(f"  - {mod}")
    if len(attention_modules) > 5:
        print(f"  ... è¿˜æœ‰ {len(attention_modules) - 5} ä¸ª")
    
    # MLPæ¨¡å—  
    mlp_modules = [mod['name'] for mod in all_linear_like if 'mlp' in mod['name']]
    print(f"\nMLPæ¨¡å— ({len(mlp_modules)}ä¸ª):")
    for mod in mlp_modules[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
        print(f"  - {mod}")
    if len(mlp_modules) > 5:
        print(f"  ... è¿˜æœ‰ {len(mlp_modules) - 5} ä¸ª")
    
    # å…¶ä»–é‡è¦æ¨¡å—
    other_modules = [mod['name'] for mod in all_linear_like if 'attn' not in mod['name'] and 'mlp' not in mod['name']]
    print(f"\nå…¶ä»–çº¿æ€§æ¨¡å— ({len(other_modules)}ä¸ª):")
    for mod in other_modules:
        print(f"  - {mod}")
    
    print("\n" + "=" * 80)
    print("ğŸ“ å»ºè®®çš„é…ç½®:")
    print("æœ€å°é…ç½®ï¼ˆä»…ç¬¬ä¸€å±‚æ³¨æ„åŠ›ï¼‰:")
    if attention_modules:
        print(f"  target_modules: ['{attention_modules[0]}']")
    
    print("\næ ‡å‡†é…ç½®ï¼ˆå¤šå±‚æ³¨æ„åŠ›ï¼‰:")
    if len(attention_modules) >= 4:
        print("  target_modules: [")
        for mod in attention_modules[:4]:
            print(f"    '{mod}',")
        print("  ]")
    
    print("\nå®Œæ•´é…ç½®ï¼ˆæ³¨æ„åŠ›+MLPï¼‰:")
    if attention_modules and mlp_modules:
        all_targets = attention_modules[:2] + mlp_modules[:2]
        print("  target_modules: [")
        for mod in all_targets:
            print(f"    '{mod}',")
        print("  ]")

if __name__ == "__main__":
    inspect_gpt2_modules()