#!/usr/bin/env python3
"""
è¯¦ç»†çš„LoRAæµ‹è¯•è„šæœ¬ - éªŒè¯LoRAåœ¨GPT2ä¸Šçš„å…·ä½“åº”ç”¨

è¯¥è„šæœ¬ä¸“é—¨æµ‹è¯•LoRAåŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
1. æ£€æŸ¥GPT2æ¨¡å—ç»“æ„
2. æ­£ç¡®åº”ç”¨LoRA
3. éªŒè¯å‚æ•°æ•°é‡å˜åŒ–
4. æµ‹è¯•è®­ç»ƒæ•ˆæœ

Author: STRATO-PEFT Research Team
Date: 2024
"""

import sys
import logging
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent / "src"))

import torch
from omegaconf import OmegaConf
from transformers import AutoTokenizer, AutoModelForCausalLM

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def analyze_gpt2_structure():
    """åˆ†æGPT2æ¨¡å‹ç»“æ„"""
    logger.info("ğŸ” åˆ†æGPT2æ¨¡å‹ç»“æ„...")
    
    model = AutoModelForCausalLM.from_pretrained("gpt2")
    
    logger.info("GPT2æ¨¡å‹çš„æ‰€æœ‰æ¨¡å—:")
    for name, module in model.named_modules():
        if isinstance(module, torch.nn.Linear):
            logger.info(f"  Linearå±‚: {name} - {module}")
            if len(name.split('.')) <= 3:  # åªæ˜¾ç¤ºä¸»è¦å±‚çº§
                logger.info(f"    è¾“å…¥ç»´åº¦: {module.in_features}, è¾“å‡ºç»´åº¦: {module.out_features}")
    
    return model

def test_lora_with_correct_modules():
    """ä½¿ç”¨æ­£ç¡®çš„æ¨¡å—åç§°æµ‹è¯•LoRA"""
    logger.info("ğŸ§ª ä½¿ç”¨æ­£ç¡®çš„æ¨¡å—åç§°æµ‹è¯•LoRA...")
    
    # åŠ è½½æ¨¡å‹å’Œtokenizer
    model = AutoModelForCausalLM.from_pretrained("gpt2")
    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # å¯¼å…¥LoRAç›¸å…³ç±»
    from src.peft.lora import LoRAPEFT
    
    # åˆ›å»ºæ­£ç¡®çš„LoRAé…ç½® - ä½¿ç”¨GPT2å®é™…çš„æ¨¡å—å
    config = OmegaConf.create({
        'rank': 8,
        'alpha': 16,
        'dropout': 0.1,
        'target_modules': ['transformer.h.0.attn.c_attn', 'transformer.h.0.attn.c_proj']  # GPT2çš„å®é™…Conv1Dæ¨¡å—å
    })
    
    logger.info(f"LoRAé…ç½®: {config}")
    
    # åº”ç”¨LoRA
    original_params = sum(p.numel() for p in model.parameters())
    logger.info(f"åŸå§‹æ¨¡å‹å‚æ•°æ•°é‡: {original_params:,}")
    
    lora_adapter = LoRAPEFT(config=config, model=model)
    adapted_model = lora_adapter.apply_peft()
    
    # æ£€æŸ¥ç»“æœ
    trainable_params = lora_adapter.get_trainable_parameters()
    trainable_count = sum(p.numel() for p in trainable_params)
    
    logger.info(f"LoRAé€‚é…å:")
    logger.info(f"  å¯è®­ç»ƒå‚æ•°: {trainable_count:,}")
    logger.info(f"  å¯è®­ç»ƒæ¯”ä¾‹: {trainable_count/original_params*100:.4f}%")
    
    # æµ‹è¯•è®­ç»ƒæ­¥éª¤
    text = "The future of artificial intelligence is"
    inputs = tokenizer(text, return_tensors="pt", max_length=32, truncation=True, padding="max_length")
    inputs["labels"] = inputs["input_ids"].clone()
    
    # å‰å‘ä¼ æ’­
    adapted_model.train()
    outputs = adapted_model(**inputs)
    loss = outputs.loss
    
    logger.info(f"å‰å‘ä¼ æ’­æˆåŠŸï¼Œloss: {loss.item():.4f}")
    
    # åå‘ä¼ æ’­
    optimizer = torch.optim.AdamW(trainable_params, lr=1e-4)
    optimizer.zero_grad()
    loss.backward()
    
    # æ£€æŸ¥æ¢¯åº¦
    grad_norms = []
    for param in trainable_params:
        if param.grad is not None:
            grad_norms.append(param.grad.norm().item())
    
    logger.info(f"LoRAå‚æ•°æ¢¯åº¦èŒƒæ•°: {grad_norms}")
    
    optimizer.step()
    logger.info("âœ… LoRAè®­ç»ƒæ­¥éª¤æˆåŠŸ")
    
    return adapted_model, lora_adapter

def test_lora_on_multiple_layers():
    """åœ¨å¤šä¸ªå±‚ä¸Šæµ‹è¯•LoRA"""
    logger.info("ğŸ§ª åœ¨å¤šä¸ªå±‚ä¸Šæµ‹è¯•LoRA...")
    
    # åŠ è½½æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained("gpt2")
    
    # åˆ›å»ºé…ç½®ï¼Œé’ˆå¯¹å¤šä¸ªtransformerå±‚
    config = OmegaConf.create({
        'rank': 4,
        'alpha': 8,
        'dropout': 0.0,
        'target_modules': [
            'transformer.h.0.attn.c_attn',
            'transformer.h.0.attn.c_proj', 
            'transformer.h.1.attn.c_attn',
            'transformer.h.1.attn.c_proj'
        ]
    })
    
    from src.peft.lora import LoRAPEFT
    
    lora_adapter = LoRAPEFT(config=config, model=model)
    adapted_model = lora_adapter.apply_peft()
    
    # ç»Ÿè®¡
    metrics = lora_adapter.get_peft_metrics()
    logger.info(f"å¤šå±‚LoRAåº”ç”¨ç»“æœ:")
    logger.info(f"  å¯è®­ç»ƒå‚æ•°: {metrics.trainable_params:,}")
    logger.info(f"  å¯è®­ç»ƒæ¯”ä¾‹: {metrics.trainable_ratio*100:.4f}%")
    logger.info(f"  é€‚é…æ•ˆç‡: {metrics.adaptation_efficiency:.4f}")
    
    return adapted_model, lora_adapter

def benchmark_lora_vs_full_training():
    """å¯¹æ¯”LoRAå’Œå…¨é‡è®­ç»ƒçš„æ€§èƒ½"""
    logger.info("ğŸ å¯¹æ¯”LoRAå’Œå…¨é‡è®­ç»ƒæ€§èƒ½...")
    
    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # æµ‹è¯•æ•°æ®
    text = "Machine learning is revolutionizing the way we understand"
    inputs = tokenizer(text, return_tensors="pt", max_length=32, truncation=True, padding="max_length")
    inputs["labels"] = inputs["input_ids"].clone()
    
    # 1. å…¨é‡è®­ç»ƒ
    logger.info("æµ‹è¯•å…¨é‡è®­ç»ƒ...")
    full_model = AutoModelForCausalLM.from_pretrained("gpt2")
    full_params = sum(p.numel() for p in full_model.parameters() if p.requires_grad)
    
    full_model.train()
    outputs = full_model(**inputs)
    full_loss = outputs.loss.item()
    logger.info(f"å…¨é‡è®­ç»ƒ - å‚æ•°: {full_params:,}, Loss: {full_loss:.4f}")
    
    # 2. LoRAè®­ç»ƒ
    logger.info("æµ‹è¯•LoRAè®­ç»ƒ...")
    lora_model = AutoModelForCausalLM.from_pretrained("gpt2")
    
    config = OmegaConf.create({
        'rank': 8,
        'alpha': 16,
        'dropout': 0.1,
        'target_modules': ['transformer.h.0.attn.c_attn', 'transformer.h.0.attn.c_proj']
    })
    
    from src.peft.lora import LoRAPEFT
    lora_adapter = LoRAPEFT(config=config, model=lora_model)
    lora_model = lora_adapter.apply_peft()
    
    lora_params = sum(p.numel() for p in lora_adapter.get_trainable_parameters())
    
    lora_model.train()
    outputs = lora_model(**inputs)
    lora_loss = outputs.loss.item()
    logger.info(f"LoRAè®­ç»ƒ - å‚æ•°: {lora_params:,}, Loss: {lora_loss:.4f}")
    
    # æ•ˆç‡å¯¹æ¯”
    param_reduction = (full_params - lora_params) / full_params * 100
    logger.info(f"å‚æ•°å‡å°‘: {param_reduction:.2f}%")
    logger.info(f"å‚æ•°æ•ˆç‡: {lora_params / full_params:.6f}")

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹è¯¦ç»†LoRAæµ‹è¯•...")
    
    try:
        # 1. åˆ†æGPT2ç»“æ„
        model = analyze_gpt2_structure()
        
        # 2. æµ‹è¯•æ­£ç¡®çš„LoRAåº”ç”¨
        adapted_model, adapter = test_lora_with_correct_modules()
        
        # 3. æµ‹è¯•å¤šå±‚LoRA
        multi_model, multi_adapter = test_lora_on_multiple_layers()
        
        # 4. æ€§èƒ½å¯¹æ¯”
        benchmark_lora_vs_full_training()
        
        logger.info("ğŸ‰ æ‰€æœ‰LoRAæµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()